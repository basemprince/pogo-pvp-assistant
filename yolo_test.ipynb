{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QLabel\n",
    "from PyQt5.QtCore import QTimer\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "from adbutils import adb\n",
    "import scrcpy.core as scrcpy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel 3 XL\n"
     ]
    }
   ],
   "source": [
    "phones = ['Pixel 3 XL', 'Pixel 7 Pro']\n",
    "adb.connect(\"127.0.0.1:5037\")\n",
    "client = scrcpy.Client(device=adb.device_list()[0])\n",
    "client.start(threaded=True)\n",
    "print(client.device_name)\n",
    "phone_t = phones.index(client.device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QSocketNotifier: Can only be used with threads started with QThread\n",
      "\n",
      "0: 640x320 (no detections), 35.1ms\n",
      "Speed: 3.4ms preprocess, 35.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[ultralytics.yolo.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.yolo.engine.results.Boxes object\n",
      "keypoints: None\n",
      "keys: ['boxes']\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "orig_img: array([[[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8)\n",
      "orig_shape: (2960, 1440)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "speed: {'preprocess': 3.395557403564453, 'inference': 35.149574279785156, 'postprocess': 0.6883144378662109}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mupdate_ui\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m screen \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mlast_frame\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m screen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[39m# Perform YOLO inference and get results\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     screen \u001b[39m=\u001b[39m yolo_inference(screen)\n\u001b[1;32m     33\u001b[0m     scale_percent \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m     34\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(screen\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m scale_percent \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36myolo_inference\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(results)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Draw results on the image\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39;49mpred[\u001b[39m0\u001b[39m]:  \u001b[39m# assuming you have one image, so results.pred[0]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     16\u001b[0m     x1, y1, x2, y2, conf, \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m result\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pred'"
     ]
    }
   ],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "update_timer = 0\n",
    "\n",
    "def yolo_inference(image):\n",
    "    # Perform inference\n",
    "    results = model(image)\n",
    "\n",
    "    # Debug: print out the type and content of results\n",
    "    print(type(results))\n",
    "    print(results)\n",
    "    print(type(results[0].boxes))\n",
    "    print(results[0].boxes)\n",
    "    # Draw results on the image\n",
    "    for result in results.pred[0]:  # assuming you have one image, so results.pred[0]\n",
    "        result = result.tolist()\n",
    "        x1, y1, x2, y2, conf, cls = result\n",
    "        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f'{int(cls)} {conf:.2f}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_ui():\n",
    "    screen = client.last_frame\n",
    "\n",
    "    if screen is not None:\n",
    "        # Perform YOLO inference and get results\n",
    "        screen = yolo_inference(screen)\n",
    "        \n",
    "        scale_percent = 20\n",
    "        width = int(screen.shape[1] * scale_percent / 100)\n",
    "        height = int(screen.shape[0] * scale_percent / 100)\n",
    "        dim = (width, height)\n",
    "        resized_image = cv2.resize(screen, dim, interpolation=cv2.INTER_AREA)\n",
    "        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        height, width, channel = resized_image.shape\n",
    "        bytes_per_line = channel * width\n",
    "        qimage = QImage(resized_image.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(qimage)\n",
    "        screenshot_label.setPixmap(pixmap)\n",
    "\n",
    "    QTimer.singleShot(update_timer, update_ui)\n",
    "\n",
    "\n",
    "app = QApplication(sys.argv)\n",
    "window = QWidget()\n",
    "window.setWindowTitle(\"Pok√©mon Information Display\")\n",
    "layout = QVBoxLayout()\n",
    "\n",
    "\n",
    "screenshot_label = QLabel()\n",
    "layout.addWidget(screenshot_label)\n",
    "\n",
    "window.setLayout(layout)\n",
    "window.show()\n",
    "\n",
    "# Start the update loop\n",
    "QTimer.singleShot(update_timer, update_ui)\n",
    "\n",
    "app.exec_()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
